@article{Zintgraf2017,
abstract = {This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).},
archivePrefix = {arXiv},
arxivId = {1702.04595},
author = {Zintgraf, Luisa M and Cohen, Taco S and Adel, Tameem and Welling, Max},
eprint = {1702.04595},
file = {:Users/njethani/Library/Application Support/Mendeley Desktop/Downloaded/Zintgraf et al. - 2017 - Visualizing Deep Neural Network Decisions Prediction Difference Analysis.pdf:pdf},
journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
month = {feb},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Visualizing Deep Neural Network Decisions: Prediction Difference Analysis}},
url = {http://arxiv.org/abs/1702.04595},
year = {2017}
}
@article{Zhou2015,
abstract = {Identifying functional effects of noncoding variants is a major challenge in human genetics. To predict the noncoding-variant effects de novo from sequence, we developed a deep learning-based algorithmic framework, DeepSEA (http://deepsea.princeton.edu/), that directly learns a regulatory sequence code from large-scale chromatin-profiling data, enabling prediction of chromatin effects of sequence alterations with single-nucleotide sensitivity. We further used this capability to improve prioritization of functional variants including expression quantitative trait loci (eQTLs) and disease-associated variants.},
author = {Zhou, Jian and Troyanskaya, Olga G.},
doi = {10.1038/nmeth.3547},
issn = {15487105},
journal = {Nature Methods},
keywords = {Computational biology and bioinformatics,Functional genomics,Genomic analysis,Systems biology},
month = {sep},
number = {10},
pages = {931--934},
pmid = {26301843},
publisher = {Nature Publishing Group},
title = {{Predicting effects of noncoding variants with deep learning-based sequence model}},
volume = {12},
year = {2015}
}
@inproceedings{Zeiler2014,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets. {\textcopyright} 2014 Springer International Publishing.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Zeiler, Matthew D. and Fergus, Rob},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1311.2901},
file = {:Users/njethani/Library/Application Support/Mendeley Desktop/Downloaded/Zeiler, Fergus - 2014 - Visualizing and understanding convolutional networks.pdf:pdf},
isbn = {9783319105895},
issn = {16113349},
month = {nov},
number = {PART 1},
pages = {818--833},
publisher = {Springer Verlag},
title = {Visualizing and understanding convolutional networks},
volume = {8689 LNCS},
year = {2014}
}
@article{Brakohiapa2017,
abstract = {Introduction: The cardio-thoracic ratio (CTR) and the transverse cardiac diameter (TCD) on Plain chest radiography are the two parameters commonly used to diagnose cardiomegaly and heart disease. A CTR of greater than 50{\%} on a PA film is abnormal and normally indicates cardiac or pericardial disease condition, whiles an increase of TCD from 1.5 to 2cm on two consecutive radiographs, taken at short interval, suggests possible cardiac pathology. The aim was to determine the suitability of using the same TCD and CTR to detect cardiomegaly for all age groups and genders respectively. Methods: A retrospective study involved the review of 1047 radiological images of adults aged 21 to 80 years, who had plain postero-anterior chest radiographs between January 2012 and November 2013 by 3 radiologists. Data recorded included the transverse cardiac, thoracic diameter and the cardiothoracic ratios. Descriptive analyses were carried out using the Microsoft excel 2010. Results: The mean age and standard deviation for the study population was 35.1 ± 12.7. The mean and standard deviations for the transverse cardiac diameter, thoracic diameter, and the cardiothoracic ratios for male participants were 13.08cm ± 1.2, 29.7cm ± 2.7 and 46.6{\%} ± 3.9; and 12.9 cm ± 1.3, 27.1 cm ± 2.6, and 47.8{\%} ± 4.8 for females. An increase in TCD of 1cm resulted in a CTR of greater than 50.0{\%} in all but the males aged 21-40 years. Conclusion: The study found that the same TCD and CTR values are not suitable in detecting cardiomegaly for all age groups and genders.},
author = {Brakohiapa, Edmund Kwadwo Kwakye and Botwe, Benard Ohene and Sarkodie, Benjamin Dabo and Ofori, Eric Kwesi and Coleman, Jerry},
doi = {10.11604/pamj.2017.27.201.12017},
file = {:Users/njethani/Library/Application Support/Mendeley Desktop/Downloaded/Brakohiapa et al. - 2017 - Radiographic determination of cardiomegaly using cardiothoracic ratio and transverse cardiac diameter Can one.pdf:pdf},
issn = {19378688},
journal = {Pan African Medical Journal},
keywords = {Cardiomegaly,Cardiothoracic ratio,Transverse cardiac diameter},
month = {jul},
pmid = {28904726},
publisher = {African Field Epidemiology Network},
title = {{Radiographic determination of cardiomegaly using cardiothoracic ratio and transverse cardiac diameter: Can one size fit all? Part one}},
volume = {27},
year = {2017}
}
@techreport{Athalye2018,
abstract = {We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization-based attacks, we find defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence , with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers.},
archivePrefix = {arXiv},
arxivId = {1802.00420v4},
author = {Athalye, Anish and Carlini, Nicholas and Wagner, David},
eprint = {1802.00420v4},
file = {:Users/njethani/Library/Application Support/Mendeley Desktop/Downloaded/Athalye, Carlini, Wagner - 2018 - Obfuscated Gradients Give a False Sense of Security Circumventing Defenses to Adversarial Examples.pdf:pdf},
title = {{Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples}},
url = {https://github.com/anishathalye/obfuscated-gradients},
year = {2018}
}
@techreport{,
abstract = {Adversarial training is one of the strongest defenses against adversarial attacks, but it requires adversarial examples to be generated for every mini-batch during optimization. The expense of producing these examples during training often precludes adversarial training from use on complex image datasets. In this study, we explore the mechanisms by which adversarial training improves classifier robust-ness, and show that these mechanisms can be effectively mimicked using simple regularization methods, including label smoothing and logit squeezing. Remarkably , using these simple regularization methods in combination with Gaussian noise injection, we are able to achieve strong adversarial robustness-often exceeding that of adversarial training-using no adversarial examples.},
file = {:Users/njethani/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - LABEL SMOOTHING AND LOGIT SQUEEZING A RE-PLACEMENT FOR ADVERSARIAL TRAINING.pdf:pdf},
title = {{LABEL SMOOTHING AND LOGIT SQUEEZING: A RE-PLACEMENT FOR ADVERSARIAL TRAINING?}}
}
@techreport{Blei2003,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
author = {Blei, David M and Ng, Andrew Y and Edu, Jordan@cs Berkeley},
booktitle = {Journal of Machine Learning Research},
file = {:Users/njethani/Library/Application Support/Mendeley Desktop/Downloaded/Blei, Ng, Edu - 2003 - Latent Dirichlet Allocation Michael I. Jordan.pdf:pdf},
pages = {993--1022},
title = {{Latent Dirichlet Allocation Michael I. Jordan}},
volume = {3},
year = {2003}
}
@article{Dabkowski2017,
abstract = {In this work we develop a fast saliency detection method that can be applied to any differentiable image classifier. We train a masking model to maniVisualizingpulate the scores of the classifier by masking salient parts of the input image. Our model generalises well to unseen images and requires a single forward pass to perform saliency detection, therefore suitable for use in real-time systems. We test our approach on CIFAR-10 and ImageNet datasets and show that the produced saliency maps are easily interpretable, sharp, and free of artifacts. We suggest a new metric for saliency and test our method on the ImageNet object localisation task. We achieve results outperforming other weakly supervised methods.},
archivePrefix = {arXiv},
arxivId = {1705.07857},
author = {Dabkowski, Piotr and Gal, Yarin},
eprint = {1705.07857},
file = {:Users/njethani/Library/Application Support/Mendeley Desktop/Downloaded/Dabkowski, Gal - 2017 - Real Time Image Saliency for Black Box Classifiers.pdf:pdf},
month = {may},
title = {Real Time Image Saliency for Black Box Classifiers},
url = {http://arxiv.org/abs/1705.07857},
year = {2017}
}
@techreport{Lee,
abstract = {While variational dropout approaches have been shown to be effective for network sparsification, they are still suboptimal in the sense that they set the dropout rate for each neuron without consideration of the input data. With such input-independent dropout, each neuron is evolved to be generic across inputs, which makes it difficult to sparsify networks without accuracy loss. To overcome this limitation, we propose adaptive variational dropout whose probabilities are drawn from sparsity-inducing beta-Bernoulli prior. It allows each neuron to be evolved either to be generic or specific for certain inputs, or dropped altogether. Such input-adaptive sparsity-inducing dropout allows the resulting network to tolerate larger degree of sparsity without losing its expressive power by removing redundancies among features. We validate our dependent variational beta-Bernoulli dropout on multiple public datasets, on which it obtains significantly more compact networks than baseline methods, with consistent accuracy improvements over the base networks.},
archivePrefix = {arXiv},
arxivId = {1805.10896v3},
author = {Lee, Juho and Kim, Saehoon and Yoon, Jaehong and Lee, Hae Beom and Yang, Eunho and Hwang, Sung Ju},
eprint = {1805.10896v3},
file = {:Users/njethani/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - Unknown - Adaptive Network Sparsification with Dependent Variational Beta-Bernoulli Dropout.pdf:pdf},
title = {{Adaptive Network Sparsification with Dependent Variational Beta-Bernoulli Dropout}}
}
@article{Taghanaki2019,
abstract = {The scarcity of richly annotated medical images is limiting supervised deep learning based solutions to medical image analysis tasks, such as localizing discriminatory radiomic disease signatures. Therefore, it is desirable to leverage unsupervised and weakly supervised models. Most recent weakly supervised localization methods apply attention maps or region proposals in a multiple instance learning formulation. While attention maps can be noisy, leading to erroneously highlighted regions, it is not simple to decide on an optimal window/bag size for multiple instance learning approaches. In this paper, we propose a learned spatial masking mechanism to filter out irrelevant background signals from attention maps. The proposed method minimizes mutual information between a masked variational representation and the input while maximizing the information between the masked representation and class labels. This results in more accurate localization of discriminatory regions. We tested the proposed model on the ChestX-ray8 dataset to localize pneumonia from chest X-ray images without using any pixel-level or bounding-box annotations.},
author = {Taghanaki, Saeid Asgari and Havaei, Mohammad and Berthier, Tess and Dutil, Francis and {Di Jorio}, Lisa and Hamarneh, Ghassan and Bengio, Yoshua},
doi = {10.1007/978-3-030-32226-7_82},
file = {:Users/njethani/Library/Application Support/Mendeley Desktop/Downloaded/Taghanaki et al. - 2019 - InfoMask Masked Variational Latent Representation to Localize Chest Disease.pdf:pdf},
keywords = {Disease localization,Mutual information,Variational representation},
title = {{InfoMask: Masked Variational Latent Representation to Localize Chest Disease}},
url = {https://doi.org/10.1007/978-3-030-32226-7{\_}82},
year = {2019}
}
@techreport{Deng,
abstract = {Neural attention has become central to many state-of-the-art models in natural language processing and related domains. Attention networks are an easy-to-train and effective method for softly simulating alignment; however, the approach does not marginalize over latent alignments in a probabilistic sense. This property makes it difficult to compare attention to other alignment approaches, to compose it with probabilistic models, and to perform posterior inference conditioned on observed data. A related latent approach, hard attention, fixes these issues, but is generally harder to train and less accurate. This work considers variational attention networks , alternatives to soft and hard attention for learning latent variable alignment models, with tighter approximation bounds based on amortized variational inference. We further propose methods for reducing the variance of gradients to make these approaches computationally feasible. Experiments show that for machine translation and visual question answering, inefficient exact latent variable models outperform standard neural attention, but these gains go away when using hard attention based training. On the other hand, variational attention retains most of the performance gain but with training speed comparable to neural attention.},
archivePrefix = {arXiv},
arxivId = {1807.03756v2},
author = {Deng, Yuntian and Kim, Yoon and Chiu, Justin and Guo, Demi and Rush, Alexander M},
eprint = {1807.03756v2},
file = {:Users/njethani/Library/Application Support/Mendeley Desktop/Downloaded/Deng et al. - Unknown - Latent Alignment and Variational Attention.pdf:pdf},
title = {{Latent Alignment and Variational Attention}},
url = {https://github.com/harvardnlp/var-attn/.}
}
@techreport{Carbonneau,
abstract = {Multiple instance learning (MIL) is a form of weakly supervised learning where training instances are arranged in sets, called bags, and a label is provided for the entire bag. This formulation is gaining interest because it naturally fits various problems and allows to leverage weakly labeled data. Consequently, it has been used in diverse application fields such as computer vision and document classification. However, learning from bags raises important challenges that are unique to MIL. This paper provides a comprehensive survey of the characteristics which define and differentiate the types of MIL problems. Until now, these problem characteristics have not been formally identified and described. As a result, the variations in performance of MIL algorithms from one data set to another are difficult to explain. In this paper, MIL problem characteristics are grouped into four broad categories: the composition of the bags, the types of data distribution, the ambiguity of instance labels, and the task to be performed. Methods specialized to address each category are reviewed. Then, the extent to which these characteristics manifest themselves in key MIL application areas are described. Finally, experiments are conducted to compare the performance of 16 state-of-the-art MIL methods on selected problem characteristics. This paper provides insight on how the problem characteristics affect MIL algorithms, recommendations for future benchmarking and promising avenues for research.},
archivePrefix = {arXiv},
arxivId = {1612.03365v1},
author = {Carbonneau, Marc-Andr{\'{e}} and Cheplygina, Veronika and Granger, Eric and Gagnon, Ghyslain},
eprint = {1612.03365v1},
file = {:Users/njethani/Library/Application Support/Mendeley Desktop/Downloaded/Carbonneau et al. - Unknown - Multiple Instance Learning A Survey of Problem Characteristics and Applications.pdf:pdf},
title = {{Multiple Instance Learning: A Survey of Problem Characteristics and Applications}}
}
@article{Xie2013,
abstract = {This contribution is part of the special series of Inaugural Articles by members of the National Academy of Sciences elected in 2009. Population heterogeneity is ubiquitous in social science. The very objective of social science research is not to discover abstract and universal laws but to understand population heterogeneity. Due to population heterogeneity, causal inference with observational data in social science is impossible without strong assumptions. Researchers have long been concerned with two potential sources of bias. The first is bias in unobserved pretreatment factors affecting the outcome even in the absence of treatment. The secondis bias due to heterogeneity in treatment effects. In this article, I show how "composition bias" due to population heterogeneity evolves over time when treatment propensity is systematically associated with heterogeneous treatment effects. A form of selection bias, composition bias, arises dynamically at the aggregate level even when the classic assumption of ignorability holds true at the microlevel.},
author = {Xie, Yu},
doi = {10.1073/pnas.1303102110},
file = {:Users/njethani/Library/Application Support/Mendeley Desktop/Downloaded/Xie - 2013 - Population heterogeneity and causal inference.pdf:pdf},
issn = {00278424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
month = {apr},
number = {16},
pages = {6262--6268},
title = {{Population heterogeneity and causal inference}},
volume = {110},
year = {2013}
}
@techreport{Alaa,
abstract = {Understanding the predictions of a machine learning model can be as crucial as the model's accuracy in many application domains. However, the black-box nature of most highly-accurate (complex) models is a major hindrance to their interpretability. To address this issue, we introduce the symbolic metamodeling framework-a general methodology for interpreting predictions by converting "black-box" models into "white-box" functions that are understandable to human subjects. A symbolic metamodel is a model of a model, i.e., a surrogate model of a trained (machine learning) model expressed through a succinct symbolic expression that comprises familiar mathematical functions and can be subjected to symbolic manipulation. We parameterize metamodels using Meijer G-functions-a class of complex-valued contour integrals that depend on real-valued parameters, and whose solutions reduce to familiar algebraic, analytic and closed-form functions for different parameter settings. This parameterization enables efficient optimization of metamodels via gradient descent, and allows discovering the functional forms learned by a model with minimal a priori assumptions. We show that symbolic metamodeling provides a generalized framework for model interpretation-many common forms of model explanation can be analytically derived from a symbolic metamodel.},
author = {Alaa, Ahmed M and {Van Der Schaar}, Mihaela},
file = {:Users/njethani/Library/Application Support/Mendeley Desktop/Downloaded/Alaa, Van Der Schaar - Unknown - Demystifying Black-box Models with Symbolic Metamodels.pdf:pdf},
title = {{Demystifying Black-box Models with Symbolic Metamodels}}
}
@techreport{Carter,
abstract = {Local explanation frameworks aim to rationalize particular decisions made by a black-box prediction model. Existing techniques are often restricted to a specific type of predictor or based on input saliency, which may be undesirably sensitive to factors unrelated to the model's decision making process. We instead propose sufficient input subsets that identify minimal subsets of features whose observed values alone suffice for the same decision to be reached, even if all other input feature values are missing. General principles that globally govern a model's decision-making can also be revealed by searching for clusters of such input patterns across many data points. Our approach is conceptually straightforward, entirely model-agnostic, simply implemented using instance-wise backward selection, and able to produce more concise rationales than existing techniques. We demonstrate the utility of our interpretation method on neural network models trained on text and image data.},
author = {Carter˚, Brandon Carter˚mit and Csail, Carter˚mit and Mueller, Jonas and Jain, Siddhartha and Gifford, David},
file = {:Users/njethani/Library/Application Support/Mendeley Desktop/Downloaded/Carter˚ et al. - Unknown - Local and global model interpretability via backward selection and clustering.pdf:pdf},
title = {{Local and global model interpretability via backward selection and clustering}},
url = {https://github.com/b-carter/sis{\_}interpretability}
}
@article{Tansey2018a,
abstract = {We consider the problem of feature selection using black box predictive models. For example, high-throughput devices in science are routinely used to gather thousands of features for each sample in an experiment. The scientist must then sift through the many candidate features to find explanatory signals in the data, such as which genes are associated with sensitivity to a prospective therapy. Often, predictive models are used for this task: the model is fit, error on held out data is measured, and strong performing models are assumed to have discovered some fundamental properties of the system. A model-specific heuristic is then used to inspect the model parameters and rank important features, with top features reported as "discoveries." However, such heuristics provide no statistical guarantees and can produce unreliable results. We propose the holdout randomization test (HRT) as a principled approach to feature selection using black box predictive models. The HRT is model agnostic and produces a valid p-value for each feature, enabling control over the false discovery rate (or Type I error) for any predictive model. Further, the HRT is computationally efficient and, in simulations, has greater power than a competing knockoffs-based approach. Code is available at https://github.com/tansey/hrt.},
archivePrefix = {arXiv},
arxivId = {1811.00645},
author = {Tansey, Wesley and Veitch, Victor and Zhang, Haoran and Rabadan, Raul and Blei, David M.},
eprint = {1811.00645},
file = {:Users/njethani/Library/Application Support/Mendeley Desktop/Downloaded/Tansey et al. - 2018 - The Holdout Randomization Test Principled and Easy Black Box Feature Selection.pdf:pdf},
month = {nov},
title = {{The Holdout Randomization Test: Principled and Easy Black Box Feature Selection}},
url = {http://arxiv.org/abs/1811.00645},
year = {2018}
}
@article{Candes2016,
abstract = {Many contemporary large-scale applications involve building interpretable models linking a large set of potential covariates to a response in a nonlinear fashion, such as when the response is binary. Although this modeling problem has been extensively studied, it remains unclear how to effectively control the fraction of false discoveries even in high-dimensional logistic regression, not to mention general high-dimensional nonlinear models. To address such a practical problem, we propose a new framework of {\$}model{\$}-{\$}X{\$} knockoffs, which reads from a different perspective the knockoff procedure (Barber and Cand$\backslash$`es, 2015) originally designed for controlling the false discovery rate in linear models. Whereas the knockoffs procedure is constrained to homoscedastic linear models with {\$}n\backslashge p{\$}, the key innovation here is that model-X knockoffs provide valid inference from finite samples in settings in which the conditional distribution of the response is arbitrary and completely unknown. Furthermore, this holds no matter the number of covariates. Correct inference in such a broad setting is achieved by constructing knockoff variables probabilistically instead of geometrically. To do this, our approach requires the covariates be random (independent and identically distributed rows) with a distribution that is known, although we provide preliminary experimental evidence that our procedure is robust to unknown/estimated distributions. To our knowledge, no other procedure solves the {\$}controlled{\$} variable selection problem in such generality, but in the restricted settings where competitors exist, we demonstrate the superior power of knockoffs through simulations. Finally, we apply our procedure to data from a case-control study of Crohn's disease in the United Kingdom, making twice as many discoveries as the original analysis of the same data.},
archivePrefix = {arXiv},
arxivId = {1610.02351},
author = {Candes, Emmanuel and Fan, Yingying and Janson, Lucas and Lv, Jinchi},
eprint = {1610.02351},
file = {:Users/njethani/Library/Application Support/Mendeley Desktop/Downloaded/Candes et al. - 2016 - Panning for Gold Model-X Knockoffs for High-dimensional Controlled Variable Selection.pdf:pdf},
month = {oct},
title = {{Panning for Gold: Model-X Knockoffs for High-dimensional Controlled Variable Selection}},
url = {http://arxiv.org/abs/1610.02351},
year = {2016}
}
@article{Fan2008,
abstract = {Variable selection plays an important role in high dimensional statistical modeling which nowadays appears in many areas and is key to various scientific discoveries. For problems of large scale or dimensionality {\$}p{\$}, estimation accuracy and computational cost are two top concerns. In a recent paper, Candes and Tao (2007) propose the Dantzig selector using {\$}L{\_}1{\$} regularization and show that it achieves the ideal risk up to a logarithmic factor {\$}\backslashlog p{\$}. Their innovative procedure and remarkable result are challenged when the dimensionality is ultra high as the factor {\$}\backslashlog p{\$} can be large and their uniform uncertainty principle can fail. Motivated by these concerns, we introduce the concept of sure screening and propose a sure screening method based on a correlation learning, called the Sure Independence Screening (SIS), to reduce dimensionality from high to a moderate scale that is below sample size. In a fairly general asymptotic framework, the correlation learning is shown to have the sure screening property for even exponentially growing dimensionality. As a methodological extension, an iterative SIS (ISIS) is also proposed to enhance its finite sample performance. With dimension reduced accurately from high to below sample size, variable selection can be improved on both speed and accuracy, and can then be accomplished by a well-developed method such as the SCAD, Dantzig selector, Lasso, or adaptive Lasso. The connections of these penalized least-squares methods are also elucidated.},
author = {Fan, Jianqing and Lv, Jinchi},
doi = {10.1111/j.1467-9868.2008.00674.x},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Adaptive lasso,Dantzig selector,Dimensionality reduction,Lasso,Oracle estimator,Smoothly clipped absolute deviation,Sure independence screening,Sure screening,Variable selection},
number = {5},
pages = {849--911},
title = {{Sure independence screening for ultrahigh dimensional feature space}},
url = {https://orfe.princeton.edu/{~}jqfan/papers/06/SIS.pdf},
volume = {70},
year = {2008}
}
@inproceedings{Shrikumar2017,
  title={Learning Important Features Through Propagating Activation Differences},
  author={Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
  booktitle={International Conference on Machine Learning},
  pages={3145--3153},
  year={2017}
}
@inproceedings{Lundberg2017,
abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
archivePrefix = {arXiv},
arxivId = {1705.07874},
author = {Lundberg, Scott M. and Lee, Su In},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1705.07874},
file = {:Users/njethani/Library/Application Support/Mendeley Desktop/Downloaded/Lundberg, Lee - 2017 - A unified approach to interpreting model predictions.pdf:pdf},
issn = {10495258},
pages = {4766--4775},
publisher = {Neural information processing systems foundation},
title = {A unified approach to interpreting model predictions},
volume = {2017-Decem},
year = {2017}
}
@article{Bang2019,
abstract = {Interpretable machine learning has gained much attention recently. Briefness and comprehensiveness are necessary in order to provide a large amount of information concisely when explaining a black-box decision system. However, existing interpretable machine learning methods fail to consider briefness and comprehensiveness simultaneously, leading to redundant explanations. We propose the variational information bottleneck for interpretation, VIBI, a system-agnostic interpretable method that provides a brief but comprehensive explanation. VIBI adopts an information theoretic principle, information bottleneck principle, as a criterion for finding such explanations. For each instance, VIBI selects key features that are maximally compressed about an input (briefness), and informative about a decision made by a black-box system on that input (comprehensive). We evaluate VIBI on three datasets and compare with state-of-the-art interpretable machine learning methods in terms of both interpretability and fidelity evaluated by human and quantitative metrics},
archivePrefix = {arXiv},
arxivId = {1902.06918},
author = {Bang, Seojin and Xie, Pengtao and Lee, Heewook and Wu, Wei and Xing, Eric},
eprint = {1902.06918},
file = {:Users/njethani/Library/Application Support/Mendeley Desktop/Downloaded/Bang et al. - 2019 - Explaining a black-box using Deep Variational Information Bottleneck Approach.pdf:pdf},
month = {feb},
title = {{Explaining a black-box using Deep Variational Information Bottleneck Approach}},
url = {http://arxiv.org/abs/1902.06918},
year = {2019}
}
@inproceedings{Chen2018,
  title={Learning to Explain: An Information-Theoretic Perspective on Model Interpretation},
  author={Chen, Jianbo and Song, Le and Wainwright, Martin and Jordan, Michael},
  booktitle={International Conference on Machine Learning},
  pages={883--892},
  year={2018}
}
@inproceedings{
yoon2018invase,
title={INVASE: Instance-wise Variable Selection using Neural Networks},
author={Jinsung Yoon and James Jordon and Mihaela van der Schaar},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=BJg_roAcK7},
}
@techreport{Yeh,
abstract = {We consider objective evaluation measures of saliency explanations for complex black-box machine learning models. We propose simple robust variants of two notions that have been considered in recent literature: (in)fidelity, and sensitivity. We analyze optimal explanations with respect to both these measures, and while the optimal explanation for sensitivity is a vacuous constant explanation, the optimal explanation for infidelity is a novel combination of two popular explanation methods. By varying the perturbation distribution that defines infidelity, we obtain novel explanations by optimizing infidelity, which we show to out-perform existing explanations in both quantitative and qualitative measurements. Another salient question given these measures is how to modify any given explanation to have better values with respect to these measures. We propose a simple modification based on lowering sensitivity, and moreover show that when done appropriately, we could simultaneously improve both sensitivity as well as fidelity.},
archivePrefix = {arXiv},
arxivId = {1901.09392v4},
author = {Yeh, Chih-Kuan and Hsieh, Cheng-Yu and {Sai Suggala}, Arun and Inouye, David I and Ravikumar, Pradeep},
eprint = {1901.09392v4},
file = {:Users/njethani/Library/Application Support/Mendeley Desktop/Downloaded/Yeh et al. - Unknown - On the (In)fidelity and Sensitivity of Explanations.pdf:pdf},
title = {{On the (In)fidelity and Sensitivity of Explanations}},
url = {https://github.com/chihkuanyeh/saliency{\_}evaluation.}
}
@misc{Sudarshan,
author = {Sudarshan, Mukund and Ranganath, Rajesh},
file = {:Users/njethani/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Model-Agnostic Feature Selection with Additional Mutual Information OpenReview.pdf:pdf},
title = {{Model-Agnostic Feature Selection with Additional Mutual Information | OpenReview}},
url = {https://openreview.net/forum?id=HJg{\_}tkBtwS},
urldate = {2019-10-23}
}
@article{Leordeanu,
author = {Leordeanu, Marius},
file = {:Users/njethani/Downloads/file.pdf:pdf},
title = {{Smoothing-based Optimization}}
}
@inproceedings{Ribeiro2016,
  title={" Why should I trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016}
}
@article{Simonyan2013,
abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
archivePrefix = {arXiv},
arxivId = {1312.6034},
author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
eprint = {1312.6034},
file = {:Users/njethani/Library/Application Support/Mendeley Desktop/Downloaded/Simonyan, Vedaldi, Zisserman - 2013 - Deep Inside Convolutional Networks Visualising Image Classification Models and Saliency Maps(3).pdf:pdf},
journal = {2nd International Conference on Learning Representations, ICLR 2014 - Workshop Track Proceedings},
mendeley-groups = {IWFS},
month = {dec},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps}},
url = {http://arxiv.org/abs/1312.6034},
year = {2013}
}
@article{Springenberg2014,
abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
archivePrefix = {arXiv},
arxivId = {1412.6806},
author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
eprint = {1412.6806},
file = {:Users/njethani/Library/Application Support/Mendeley Desktop/Downloaded/Springenberg et al. - 2014 - Striving for Simplicity The All Convolutional Net(2).pdf:pdf},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Workshop Track Proceedings},
mendeley-groups = {IWFS},
month = {dec},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Striving for Simplicity: The All Convolutional Net}},
url = {http://arxiv.org/abs/1412.6806},
year = {2014}
}
@article{LeCun1998,
abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient-based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN's), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day. {\textcopyright} 1998 IEEE.},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
mendeley-groups = {IWFS},
number = {11},
pages = {2278--2323},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@article{Glynn1990,
abstract = {Consider a computer system having a CPU that feeds jobs to two input/output 1990 devices having different speeds. Let $\theta$ be the fraction of jobs routed to the first I/O device, so that 1 - $\theta$ is the fraction routed to the second. Suppose that $\alpha$ = $\alpha$($\theta$) is the steady-sate amount of time that a job spends in the system. Given that $\theta$ is a decision variable, a designer might wish to minimize $\alpha$($\theta$) over $\theta$. Since $\alpha$({\textperiodcentered}) is typically difficult to evaluate analytically, Monte Carlo optimization is an attractive methodology. By analogy with deterministic mathematical programming, efficient Monte Carlo gradient estimation is an important ingredient of simulation-based optimization algorithms. As a consequence, gradient estimation has recently attracted considerable attention in the simulation community. It is our goal, in this article, to describe one efficient method for estimating gradients in the Monte Carlo setting, namely the likelihood ratio method (also known as the efficient score method). This technique has been previously described (in less general settings than those developed in this article) in [6, 16, 18, 21]. An alternative gradient estimation procedure is infinitesimal perturbation analysis; see [11, 12] for an introduction. While it is typically more difficult to apply to a given application than the likelihood ratio technique of interest here, it often turns out to be statistically more accurate. In this article, we first describe two important problems which motivate our study of efficient gradient estimation algorithms. Next, we will present the likelihood ratio gradient estimator in a general setting in which the essential idea is most transparent. The section that follows then specializes the estimator to discrete-time stochastic processes. We derive likelihood-ratio-gradient estimators for both time-homogeneous and non-time homogeneous discrete-time Markov chains. Later, we discuss likelihood ratio gradient estimation in continuous time. As examples of our analysis, we present the gradient estimators for time-homogeneous continuous-time Markov chains; non-time homogeneous continuous-time Markov chains; semi-Markov processes; and generalized semi-Markov processes. (The analysis throughout these sections assumes the performance measure that defines $\alpha$($\theta$) corresponds to a terminating simulation.) Finally, we conclude the article with a brief discussion of the basic issues that arise in extending the likelihood ratio gradient estimator to steady-state performance measures. {\textcopyright} 1990, ACM. All rights reserved.},
author = {Glynn, Peter W.},
doi = {10.1145/84537.84552},
issn = {15577317},
journal = {Communications of the ACM},
mendeley-groups = {IWFS},
month = {jan},
number = {10},
pages = {75--84},
title = {{Likelihood ratio gradient estimation for stochastic systems}},
url = {http://portal.acm.org/citation.cfm?doid=84537.84552},
volume = {33},
year = {1990}
}
@article{Williams1992,
abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
author = {Williams, Ronald J.},
doi = {10.1007/bf00992696},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {Artificial Intelligence,Control,Mechatronics,Natural Language Processing (NLP),Robotics,Simulation and Modeling},
mendeley-groups = {IWFS},
month = {may},
number = {3-4},
pages = {229--256},
publisher = {Springer Science and Business Media LLC},
title = {{Simple statistical gradient-following algorithms for connectionist reinforcement learning}},
volume = {8},
year = {1992}
}
@article{Maddison2016,
abstract = {The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables---continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.},
archivePrefix = {arXiv},
arxivId = {1611.00712},
author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
eprint = {1611.00712},
file = {:Users/njethani/Library/Application Support/Mendeley Desktop/Downloaded/Maddison, Mnih, Teh - 2016 - The Concrete Distribution A Continuous Relaxation of Discrete Random Variables(2).pdf:pdf},
journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
mendeley-groups = {IWFS},
month = {nov},
publisher = {International Conference on Learning Representations, ICLR},
title = {{The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables}},
url = {http://arxiv.org/abs/1611.00712},
year = {2016}
}
@misc{mohamed2019monte,
    title={Monte Carlo Gradient Estimation in Machine Learning},
    author={Shakir Mohamed and Mihaela Rosca and Michael Figurnov and Andriy Mnih},
    year={2019},
    eprint={1906.10652},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}
@inproceedings{jang2016categorical,
    title={Categorical Reparameterization with Gumbel-Softmax},
    author={Eric Jang and Shixiang Gu and Ben Poole},
    year={2017},
    booktitle={International Conference on Learning Representations}
}
@misc{bengio2013estimating,
    title={Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation},
    author={Yoshua Bengio and Nicholas Léonard and Aaron Courville},
    year={2013},
    eprint={1308.3432},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@article{Wang_2017,
   title={ChestX-Ray8: Hospital-Scale Chest X-Ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases},
   ISBN={9781538604571},
   url={http://dx.doi.org/10.1109/CVPR.2017.369},
   DOI={10.1109/cvpr.2017.369},
   journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
   publisher={IEEE},
   author={Wang, Xiaosong and Peng, Yifan and Lu, Le and Lu, Zhiyong and Bagheri, Mohammadhadi and Summers, Ronald M.},
   year={2017},
   month={Jul}
}
@incollection{NIPS2019_9167,
    title = {A Benchmark for Interpretability Methods in Deep Neural Networks},
    author = {Hooker, Sara and Erhan, Dumitru and Kindermans, Pieter-Jan and Kim, Been},
    booktitle = {Advances in Neural Information Processing Systems 32},
    editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
    pages = {9737--9748},
    year = {2019},
    publisher = {Curran Associates, Inc.},
    url = {http://papers.nips.cc/paper/9167-a-benchmark-for-interpretability-methods-in-deep-neural-networks.pdf}
}
@inproceedings{tucker2017rebar,
  title={Rebar: Low-variance, unbiased gradient estimates for discrete latent variable models},
  author={Tucker, George and Mnih, Andriy and Maddison, Chris J and Lawson, John and Sohl-Dickstein, Jascha},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2627--2636},
  year={2017}
}
@article{Selvaraju_2019,
   title={Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization},
   volume={128},
   ISSN={1573-1405},
   url={http://dx.doi.org/10.1007/s11263-019-01228-7},
   DOI={10.1007/s11263-019-01228-7},
   number={2},
   journal={International Journal of Computer Vision},
   publisher={Springer Science and Business Media LLC},
   author={Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
   year={2019},
   month={Oct},
   pages={336–359}
}
@article{ishibuchi2007analysis,
  title={Analysis of interpretability-accuracy tradeoff of fuzzy systems by multiobjective fuzzy genetics-based machine learning},
  author={Ishibuchi, Hisao and Nojima, Yusuke},
  journal={International Journal of Approximate Reasoning},
  volume={44},
  number={1},
  pages={4--31},
  year={2007},
  publisher={Elsevier}
}
@misc{lakkaraju2017interpretable,
      title={Interpretable & Explorable Approximations of Black Box Models}, 
      author={Himabindu Lakkaraju and Ece Kamar and Rich Caruana and Jure Leskovec},
      year={2017},
      eprint={1707.01154},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@article{cicero2017training,
  title={Training and validating a deep convolutional neural network for computer-aided detection and classification of abnormalities on frontal chest radiographs},
  author={Cicero, Mark and Bilbily, Alexander and Colak, Errol and Dowdell, Tim and Gray, Bruce and Perampaladas, Kuhan and Barfett, Joseph},
  journal={Investigative radiology},
  volume={52},
  number={5},
  pages={281--287},
  year={2017},
  publisher={LWW}
}
@misc{geras2018highresolution,
      title={High-Resolution Breast Cancer Screening with Multi-View Deep Convolutional Neural Networks}, 
      author={Krzysztof J. Geras and Stacey Wolfson and Yiqiu Shen and Nan Wu and S. Gene Kim and Eric Kim and Laura Heacock and Ujas Parikh and Linda Moy and Kyunghyun Cho},
      year={2018},
      eprint={1703.07047},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{gulshan2016development,
  title={Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs},
  author={Gulshan, Varun and Peng, Lily and Coram, Marc and Stumpe, Martin C and Wu, Derek and Narayanaswamy, Arunachalam and Venugopalan, Subhashini and Widner, Kasumi and Madams, Tom and Cuadros, Jorge and others},
  journal={Jama},
  volume={316},
  number={22},
  pages={2402--2410},
  year={2016},
  publisher={American Medical Association}
}
@article{de2018clinically,
  title={Clinically applicable deep learning for diagnosis and referral in retinal disease},
  author={De Fauw, Jeffrey and Ledsam, Joseph R and Romera-Paredes, Bernardino and Nikolov, Stanislav and Tomasev, Nenad and Blackwell, Sam and Askham, Harry and Glorot, Xavier and O’Donoghue, Brendan and Visentin, Daniel and others},
  journal={Nature medicine},
  volume={24},
  number={9},
  pages={1342--1350},
  year={2018},
  publisher={Nature Publishing Group}
}
@article{Ghorbani_2020,
	Author = {Ghorbani, Amirata and Ouyang, David and Abid, Abubakar and He, Bryan and Chen, Jonathan H. and Harrington, Robert A. and Liang, David H. and Ashley, Euan A. and Zou, James Y.},
	Da = {2020/01/24},
	Date-Added = {2020-10-09 00:00:53 +0000},
	Date-Modified = {2020-10-09 00:00:53 +0000},
	Doi = {10.1038/s41746-019-0216-8},
	Id = {Ghorbani2020},
	Isbn = {2398-6352},
	Journal = {npj Digital Medicine},
	Number = {1},
	Pages = {10},
	Title = {Deep learning interpretation of echocardiograms},
	Ty = {JOUR},
	Url = {https://doi.org/10.1038/s41746-019-0216-8},
	Volume = {3},
	Year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41746-019-0216-8},
	Bdsk-Url-2 = {http://dx.doi.org/10.1038/s41746-019-0216-8}}
@article{Attia2019a,
abstract = {Asymptomatic left ventricular dysfunction (ALVD) is present in 3–6{\%} of the general population, is associated with reduced quality of life and longevity, and is treatable when found1–4. An inexpensive, noninvasive screening tool for ALVD in the doctor's office is not available. We tested the hypothesis that application of artificial intelligence (AI) to the electrocardiogram (ECG), a routine method of measuring the heart's electrical activity, could identify ALVD. Using paired 12-lead ECG and echocardiogram data, including the left ventricular ejection fraction (a measure of contractile function), from 44,959 patients at the Mayo Clinic, we trained a convolutional neural network to identify patients with ventricular dysfunction, defined as ejection fraction ≤35{\%}, using the ECG data alone. When tested on an independent set of 52,870 patients, the network model yielded values for the area under the curve, sensitivity, specificity, and accuracy of 0.93, 86.3{\%}, 85.7{\%}, and 85.7{\%}, respectively. In patients without ventricular dysfunction, those with a positive AI screen were at 4 times the risk (hazard ratio, 4.1; 95{\%} confidence interval, 3.3 to 5.0) of developing future ventricular dysfunction compared with those with a negative screen. Application of AI to the ECG—a ubiquitous, low-cost test—permits the ECG to serve as a powerful screening tool in asymptomatic individuals to identify ALVD. A deep learning algorithm applied to the electrocardiogram—a test of the heart's electrical activity—can detect abnormally low contractile function of the heart, opening up the possibility for a simple screening tool for this condition.},
author = {Attia, Zachi I. and Kapa, Suraj and Lopez-Jimenez, Francisco and McKie, Paul M. and Ladewig, Dorothy J. and Satam, Gaurav and Pellikka, Patricia A. and Enriquez-Sarano, Maurice and Noseworthy, Peter A. and Munger, Thomas M. and Asirvatham, Samuel J. and Scott, Christopher G. and Carter, Rickey E. and Friedman, Paul A.},
doi = {10.1038/s41591-018-0240-2},
file = {:Users/njethani/Library/Application Support/Mendeley Desktop/Downloaded/Attia et al. - 2019 - Screening for cardiac contractile dysfunction using an artificial intelligence–enabled electrocardiogram.pdf:pdf},
issn = {1078-8956},
journal = {Nature Medicine},
keywords = {Arrhythmias,Machine learning},
mendeley-groups = {EWAS,EWAS/ECG{\_}ML},
month = {jan},
number = {1},
pages = {70--74},
publisher = {Nature Publishing Group},
title = {{Screening for cardiac contractile dysfunction using an artificial intelligence–enabled electrocardiogram}},
url = {http://www.nature.com/articles/s41591-018-0240-2},
volume = {25},
year = {2019}
}
@article{coudray2018classification,
  title={Classification and mutation prediction from non--small cell lung cancer histopathology images using deep learning},
  author={Coudray, Nicolas and Ocampo, Paolo Santiago and Sakellaropoulos, Theodore and Narula, Navneet and Snuderl, Matija and Feny{\"o}, David and Moreira, Andre L and Razavian, Narges and Tsirigos, Aristotelis},
  journal={Nature medicine},
  volume={24},
  number={10},
  pages={1559--1567},
  year={2018},
  publisher={Nature Publishing Group}
}
@article{djuric2017precision,
  title={Precision histology: how deep learning is poised to revitalize histomorphology for personalized cancer care},
  author={Djuric, Ugljesa and Zadeh, Gelareh and Aldape, Kenneth and Diamandis, Phedias},
  journal={NPJ precision oncology},
  volume={1},
  number={1},
  pages={1--5},
  year={2017},
  publisher={Nature Publishing Group}
}
@article{zech2018variable,
  title={Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study},
  author={Zech, John R and Badgeley, Marcus A and Liu, Manway and Costa, Anthony B and Titano, Joseph J and Oermann, Eric Karl},
  journal={PLoS medicine},
  volume={15},
  number={11},
  pages={e1002683},
  year={2018},
  publisher={Public Library of Science San Francisco, CA USA}
}
@misc{lipton2017mythos,
      title={The Mythos of Model Interpretability}, 
      author={Zachary C. Lipton},
      year={2017},
      eprint={1606.03490},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}
@misc{zhou2015learning,
      title={Learning Deep Features for Discriminative Localization}, 
      author={Bolei Zhou and Aditya Khosla and Agata Lapedriza and Aude Oliva and Antonio Torralba},
      year={2015},
      eprint={1512.04150},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{Salimans_2013,
   title={Fixed-Form Variational Posterior Approximation through Stochastic Linear Regression},
   volume={8},
   ISSN={1936-0975},
   url={http://dx.doi.org/10.1214/13-BA858},
   DOI={10.1214/13-ba858},
   number={4},
   journal={Bayesian Analysis},
   publisher={Institute of Mathematical Statistics},
   author={Salimans, Tim and Knowles, David A.},
   year={2013},
   month={Dec},
   pages={837–882}
}
@inproceedings{kingma2014autoencoding,
      title={Auto-Encoding Variational Bayes}, 
      author={Diederik P Kingma and Max Welling},
      year={2014},
      booktitle={International Conference on Learning Representations}
}
@article{guyon2003introduction,
  title={An introduction to variable and feature selection},
  author={Guyon, Isabelle and Elisseeff, Andr{\'e}},
  journal={Journal of machine learning research},
  volume={3},
  number={Mar},
  pages={1157--1182},
  year={2003}
}
@article{kohavi1997wrappers,
  title={Wrappers for feature subset selection},
  author={Kohavi, Ron and John, George H and others},
  journal={Artificial intelligence},
  volume={97},
  number={1-2},
  pages={273--324},
  year={1997},
  publisher={Elsevier Science}
}
@article{tibshirani1996regression,
  title={Regression shrinkage and selection via the lasso},
  author={Tibshirani, Robert},
  journal={Journal of the Royal Statistical Society: Series B (Methodological)},
  volume={58},
  number={1},
  pages={267--288},
  year={1996},
  publisher={Wiley Online Library}
}
@article{candes2016panning,
  title={Panning for gold:‘model-X’knockoffs for high dimensional controlled variable selection},
  author={Cand{\`e}s, Emmanuel and Fan, Yingying and Janson, Lucas and Lv, Jinchi},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume={80},
  number={3},
  pages={551--577},
  year={2018},
  publisher={Wiley Online Library}
}
@inproceedings{adebayo2018sanity,
  title={Sanity checks for saliency maps},
  author={Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9505--9515},
  year={2018}
}
@misc{lundberg2019consistent,
      title={Consistent Individualized Feature Attribution for Tree Ensembles}, 
      author={Scott M. Lundberg and Gabriel G. Erion and Su-In Lee},
      year={2019},
      eprint={1802.03888},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{schwab2019cxplain,
      title={CXPlain: Causal Explanations for Model Interpretation under Uncertainty}, 
      author={Patrick Schwab and Walter Karlen},
      year={2019},
      eprint={1910.12336},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{dabkowski2017real,
      title={Real Time Image Saliency for Black Box Classifiers}, 
      author={Piotr Dabkowski and Yarin Gal},
      year={2017},
      eprint={1705.07857},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@misc{kingma2014autoencoding,
      title={Auto-Encoding Variational Bayes}, 
      author={Diederik P Kingma and Max Welling},
      year={2014},
      eprint={1312.6114},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@ARTICLE{samek2017, 
author={W. Samek and A. Binder and G. Montavon and S. Lapuschkin and K. R. MÃŒller},
journal={IEEE Transactions on Neural Networks and Learning Systems}, 
title="{Evaluating the Visualization of What a Deep Neural Network Has Learned}", 
year=2017, 
month={Nov},
volume={28}, 
number={11}, 
pages={2660-2673}, 
keywords={data visualisation;image classification;learning (artificial intelligence);neural nets;DNN;ILSVRC2012;MIT Places data sets;SUN397;complex machine learning tasks;data visualization;deconvolution method;deep neural network;heatmap;multilayer nonlinear structure;relevance propagation algorithm;sensitivity-based approach;Algorithm design and analysis;Biological neural networks;Deconvolution;Heating;Learning systems;Neurons;Sensitivity;Convolutional neural networks;explaining classification;image classification;interpretable machine learning;relevance models}, 
doi={10.1109/TNNLS.2016.2599820}, 
ISSN={2162-237X},
}
@article{Attia2019a,
abstract = {Asymptomatic left ventricular dysfunction (ALVD) is present in 3–6% of the general population, is associated with reduced quality of life and longevity, and is treatable when found1–4. An inexpensive, noninvasive screening tool for ALVD in the doctor's office is not available. We tested the hypothesis that application of artificial intelligence (AI) to the electrocardiogram (ECG), a routine method of measuring the heart's electrical activity, could identify ALVD. Using paired 12-lead ECG and echocardiogram data, including the left ventricular ejection fraction (a measure of contractile function), from 44,959 patients at the Mayo Clinic, we trained a convolutional neural network to identify patients with ventricular dysfunction, defined as ejection fraction ≤35%, using the ECG data alone. When tested on an independent set of 52,870 patients, the network model yielded values for the area under the curve, sensitivity, specificity, and accuracy of 0.93, 86.3%, 85.7%, and 85.7%, respectively. In patients without ventricular dysfunction, those with a positive AI screen were at 4 times the risk (hazard ratio, 4.1; 95% confidence interval, 3.3 to 5.0) of developing future ventricular dysfunction compared with those with a negative screen. Application of AI to the ECG—a ubiquitous, low-cost test—permits the ECG to serve as a powerful screening tool in asymptomatic individuals to identify ALVD. A deep learning algorithm applied to the electrocardiogram—a test of the heart's electrical activity—can detect abnormally low contractile function of the heart, opening up the possibility for a simple screening tool for this condition.},
author = {Attia, Zachi I. and Kapa, Suraj and Lopez-Jimenez, Francisco and McKie, Paul M. and Ladewig, Dorothy J. and Satam, Gaurav and Pellikka, Patricia A. and Enriquez-Sarano, Maurice and Noseworthy, Peter A. and Munger, Thomas M. and Asirvatham, Samuel J. and Scott, Christopher G. and Carter, Rickey E. and Friedman, Paul A.},
doi = {10.1038/s41591-018-0240-2},
file = {:Users/njethani/Library/Application Support/Mendeley Desktop/Downloaded/Attia et al. - 2019 - Screening for cardiac contractile dysfunction using an artificial intelligence–enabled electrocardiogram.pdf:pdf},
issn = {1078-8956},
journal = {Nature Medicine},
keywords = {Arrhythmias,Machine learning},
mendeley-groups = {EWAS,EWAS/ECG_ML},
month = {jan},
number = {1},
pages = {70--74},
publisher = {Nature Publishing Group},
title = {Screening for cardiac contractile dysfunction using an artificial intelligence–enabled electrocardiogram},
url = {http://www.nature.com/articles/s41591-018-0240-2},
volume = {25},
year = {2019}
}
@article{major2020estimating,
  title={Estimating real-world performance of a predictive model: a case-study in predicting mortality},
  author={Major, Vincent J and Jethani, Neil and Aphinyanaphongs, Yindalon},
  journal={JAMIA open},
  volume={3},
  number={2},
  pages={243--251},
  year={2020},
  publisher={Oxford University Press}
}